\chapter{Methodik}
\section{Aufbau des Experiments}
Um zu untersuchen, wie Reinforcement-Learning-Agenten im Iterierten 
Gefangenendilemma (IPD) agieren, wurde eine experimentelle 
\href{https://github.com/Jonah-gr/Reinforcement-Learning-IPD}{Python-Umgebung} implementiert. 
Diese Umgebung ermöglicht es, Spiele zu simulieren, RL-Agenten zu trainieren, und diese dann zu evaluieren.
Für all diese Zwecke wurde die Rundenanzahl pro Spiel auf $n=100$ gesetzt und die folgende Auszahlungsmatrix verwendet:

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c}
            & Spieler B kooperiert & Spieler B defektiert\\
        \hline
        Spieler A kooperiert &  (3, 3) & (5, 0) \\
        \hline
        Spieler A defektiert &  (0, 5) & (1, 1) \\
    \end{tabular}
    \caption{Auszahlungsmatrix}
    \label{table:auszahlungsmatrix}
\end{table}

\section{Agenten und Strategien}
\label{3.2}
Für das Training und die Evaluation der RL-Agent wurden folgende Basis-Strategien implementiert\footcite{axelrod1984cooperation}\footcite{stewart2012extortion}:

\begin{longtable}{|c|m{7cm}|}
    \hline
    \textbf{Strategie} & \textbf{Beschreibung} \\
    \hline
    RandomAgent & Entscheidet zufällig \\
    \hline
    AlwaysCooperateAgent & Kooperiert immer \\
    \hline
    AlwaysDefectAgent & Verrät immer \\
    \hline
    ProvocativeAgent & Verrät nach zweimaligem Kooperieren \\
    \hline
    TitForTatAgent & Imitiert den Gegner \\
    \hline
    TitForTwoTatsAgent & Verrät, wenn der Gegner zweimal verrät \\
    \hline
    TwoTitsForTatAgent & Verrät zweimal, wenn der Gegner verrät \\
    \hline
    TitForTatOppositeAgent & Imitiert den Gegner umgekehrt \\
    \hline
    SpitefulAgent & Verrät immer, wenn der Gegner verrät \\
    \hline
    GenerousTitForTatAgent & Imitiert den Gegner, vergibt aber in 10\% der Fälle \\
    \hline
    AdaptiveAgent & Verrät, wenn der Gegner in den letzten 10 Runden zu mehr als 50\% verraten hat \\
    \hline
    PavlovAgent & Verrät, wenn die Entscheidungen in der vorherigen Runde unterschiedlich waren \\
    \hline
    GradualAgent & Verrät so oft, wie der Gegner verrät \\
    \hline
    WinStayLoseShiftAgent & Ändert die Strategie, wenn die letzte Belohnung < 1 war \\
    \hline
    SoftMajorityAgent & Verrät, wenn der Gegner in mehr als 50\% der Fälle verrät \\
    \hline
    SuspiciousTitForTatAgent & Imitiert den Gegner und verrät in der ersten Runde \\
    \hline
    SuspiciousAdaptiveAgent & Verrät, wenn der Gegner in den letzten 10 Runden zu mehr als 50\% verraten hat, und verrät in der ersten Runde \\
    \hline
    SuspiciousGenerousTitForTatAgent & Imitiert den Gegner, vergibt aber in 10\% der Fälle und verrät in der ersten Runde \\
    \hline
    SuspiciousGradualAgent & Verrät so oft, wie der Gegner verrät, und verrät in der ersten Runde \\
    \hline
    SuspiciousPavlovAgent & Verrät, wenn die Entscheidungen in der vorherigen Runde unterschiedlich waren, und verrät in der ersten Runde \\
    \hline
    SuspiciousSoftMajorityAgent & Verrät, wenn der Gegner in mehr als 50\% der Fälle verrät, und verrät in der ersten Runde \\
    \hline
    SuspiciousTitForTwoTatsAgent & Verrät, wenn der Gegner zweimal verrät, und verrät in der ersten Runde \\
    \hline
    SuspiciousTwoTitsForTatAgent & Verrät zweimal, wenn der Gegner verrät, und verrät in der ersten Runde \\
    \hline
    SuspiciousWinStayLoseShiftAgent & Ändert die Strategie, wenn die letzte Belohnung < 1 war, und verrät in der ersten Runde \\
    \hline
\caption{Basis-Strategien}
\label{table:basestrategies}
\end{longtable}
Zusätzlich gibt es den \textit{RandomStrategies} Agenten, der jede Epsiode  zufällig  eine Strategie aus den 
Basis-Strategien (siehe Tab. \ref{table:basestrategies}) auswählt und so seine
Entscheidungen trifft. \\

Der Q-Learning Agent nutzt eine $2 \times 2$ Q-Tabelle zur Approximation der Q-Werte:
\begin{table}[H]
    \centering
    \begin{tabular}{c|c|c}
            & Q-Werte für Kooperation & Q-Werte für Defektion \\
        \hline
        Gegner kooperierte &  Q(0, 0) & Q(0, 1) \\
        \hline
        Gegner defektierte &  Q(1, 0) & Q(1, 1) \\
    \end{tabular}
    \caption{Q-Tabelle}
    \label{table:qtable}
\end{table}
Weil die Q-Tabelle nur den letzten Gegnerzug betrachtet, kann der Q-Learning Agent nur vier Strategien entwickeln:
\begin{enumerate}
    \item Immer kooperieren (wie \textit{AlwaysCooperateAgent}) mit
        \begin{align*}
            Q(0, 0) > Q(0, 1) \land Q(1, 0) > Q(1, 1)
        \end{align*}
    \item Immer defektieren (wie \textit{AlwaysDefectAgent}) mit
        \begin{align*}
            Q(0, 0) < Q(0, 1) \land Q(1, 0) < Q(1, 1)
        \end{align*} 
    \item Den Gegner imitieren (wie \textit{TitForTatAgent}) mit
        \begin{align*}
            Q(0, 0) > Q(0, 1) \land Q(1, 0) < Q(1, 1)
        \end{align*}
    \item Gegenteil der Imitation (wie \textit{TitForTatOppositeAgent}) mit
        \begin{align*}
            Q(0, 0) < Q(0, 1) \land Q(1, 0) > Q(1, 1)
        \end{align*}
\end{enumerate}
Hierbei steuert die Lernrate $\alpha=0.001$, wie stark neue Informationen in die Q-Tabelle einfließen, während der 
Discount-Faktor $\gamma=0.95$ zukünftige Belohnungen mit einbezieht. Der Agent nutzt eine $\epsilon$-greedy Strategie, 
bei der mit Wahrscheinlichkeit $\epsilon$ eine zufällige Aktion gewählt wird, um Exploration zu ermöglichen. 
$\epsilon$ startet bei 1.0 und wird mit 0.9995 pro Runde reduziert, bis ein Minimum von 0.00001 erreicht ist. 
Zur Entscheidungsfindung nutzt er entweder zufällige
Exploration oder wählt die Aktion mit dem höchsten Q-Wert basierend auf der letzten Gegneraktion. \\

Der Deep Q-Learning Agent nutzt ein neuronales Netzwerk zur Approximation der Q-Werte und basiert auf einem 
Feedforward-Netzwerk mit drei voll verbundenen Schichten: Eine Eingabeschicht mit 64 Neuronen, eine versteckte 
Schicht mit 32 Neuronen und eine Ausgabeschicht mit 2 Neuronen, die die Q-Werte für die beiden möglichen Aktionen 
(Kooperation oder Defektion) liefert. ReLU-Aktivierungen sorgen für nicht-lineare Modellierung, während Xavier-Initialisierung 
eine stabile Gewichtsverteilung gewährleistet. Diese sorgt dafür, dass die Gewichte so initialisiert werden, dass der 
Informationsfluss durch das Netzwerk stabil bleibt. Die Gewichte $W$ werden so initialisiert, dass
\begin{equation}
    W \sim U \left( -\frac{\sqrt{6}}{\sqrt{n_{\text{in}}+n_{\text{out}}}}, \frac{6}{\sqrt{n_{\text{in}}+n_{\text{out}}}} \right)
\end{equation}
mit $n_{in}$ und $n_{out}$ als Anzahl der Neuronen der vorherigen und der aktuellen Schicht. Die Varianz der Gewichte bleibt
also über alle Schichten hinweg konstant.\footcite{pmlr-v9-glorot10a}
Das Netz bekommt die letzten zehn Runden, also einen 20-dimensionalen 
Zustandsvektor, als Eingabe. Der Agent nutzt eine Replay-Memory (max. 20.000 Einträge) zur stabileren 
Lernprozessgestaltung und ein $\epsilon$-greedy Explorationsverfahren, bei dem die Wahrscheinlichkeit für zufällige Aktionen mit 
zunehmendem Training abnimmt ($\epsilon$ sinkt von 1.0 auf 0.00001 mit einem Faktor von 0.9995). Das Lernen erfolgt über MSE-Loss und Adam-Optimierung mit 
einer Lernrate von 0.001.
Während des Trainings werden zufällige 5 Batches aus der Replay-Memory gezogen und die Q-Werte 
aktualisiert, wobei der jeweilige optimale Q-Wert (Ausgabe des Zielnetzwerks) mit $\gamma=0.95$ diskontiert wird. $\gamma$ ist desehalb jeweils
so hoch gewhält, damit die Agenten der Zielsetzung ensprechend langfristige Strategien entwickeln.\\ \\
Die Belohnunsfunktion beider RL-Agenten ist die vorher beschriebene Auszahlungsmatrix (siehe Tab. \ref{table:auszahlungsmatrix}).
Beide RL-Agenten wurden in 10000 Episoden gegen eine zufällige aber feste Reihenfolge von Basis-Agenten trainiert.


\section{Evaluierungsmethodik}
Die Evaluierungsmethodik basiert auf einem Turnierformat, in dem jeder Agent gegen jeden Basis-Agenten 100 Spiele 
absolvieren muss. Jeder Agent spielt also $24 * 100 = 2400$ Spiele (es gibt 24 Basis-Agenten) und $2400 * 100 = 240000$ Runden. 
Am Ende werden die Gesamtbelohnungen aller Agenten über alle Spiele hinweg miteinander verglichen.
Dieses Format stellt sicher, dass für alle Agenten die selben Voraussetzungen herrschen und statistisch signifante Ergebnisse erzielt werden.
