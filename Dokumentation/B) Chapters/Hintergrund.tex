\chapter{Hintergrund und theoretischer Rahmen}
\section{Das Iterierter Gefangenendilemma}
Das Gefangenendilemma ist eines der bekanntesten Probleme der Spieltheorie und 
beschreibt eine Situation, in der zwei Spieler unabhängig voneinander entscheiden 
müssen, ob sie kooperieren oder defektieren. Die Auszahlung für jeden Spieler hängt 
sowohl von der eigenen Entscheidung als auch von der des Gegenübers ab. 
Die klassische Auszahlungsmatrix sieht dabei folgendermaßen aus: \newline \newline 
% \begin{table}
\begin{tabular}{c|c|c}
        & Spieler B kooperiert & Spieler B defektiert\\
    \hline \\
    Spieler A kooperiert &  \makecell{(R, R) \\ Belohnung für Kooperation} & \makecell{(S, T) \\ A verliert, B gewinnt} \\
    \hline \\
    Spieler A defektiert &  \makecell{(T, S) \\ A gewinnt, B verliert} & \makecell{(P, P) \\ Bestrafung für gegenseitige Defektion} \\
\end{tabular}
% \end{table}
Dabei gilt üblicherweise:
\begin{itemize}
    \item T (Temptation) > R (Reward) > P (Punishment) > S (Sucker's payoff)
    \item 2R > T + S, sodass sich gegenseitige Kooperation langfristig mehr lohnen würde als wechselseitige Ausnutzung.
\end{itemize}
Im einmaligen Gefangenendilemma ist die dominante Strategie, zu defektieren, da 
dies in jedem individuellen Fall die höhere Auszahlung sichert – unabhängig von der 
Entscheidung des Gegenspielers. Dies führt jedoch zu einem sozial suboptimalen 
Ergebnis.

Im iterierten Gefangenendilemma (IDG) wird das Spiel jedoch mehrfach hintereinander 
gespielt, sodass frühere Entscheidungen zukünftige Interaktionen beeinflussen können. 
Dadurch eröffnen sich neue Möglichkeiten für kooperative Strategien, bei denen 
Agenten versuchen, durch wechselseitige Zusammenarbeit langfristig höhere Erträge 
zu erzielen. Bekannte Strategien aus der Spieltheorie für das IDG sind beispielsweise:
"Tit-for-Tat" (Spiele das, was dein Gegner in der vorherigen Runde getan hat).
"Always Defect" (Immer defektieren, um kurzfristig die höchste Auszahlung zu sichern).
"Grim Trigger" (Kooperiere, aber falls der Gegner einmal defektiert, defektiere für immer).

Die zentrale Forschungsfrage im IPD lautet daher: Ist es möglich, langfristige 
Kooperation zu etablieren, oder führt Eigennutz immer zu gegenseitiger Defektion? 
Diese Fragestellung ist besonders relevant für Reinforcement Learning-Agenten, da 
sie ihre Strategie durch wiederholte Interaktion und Belohnungsmechanismen erlernen.

\section{Reinforcement Learning}
Reinforcement Learning (RL) ist ein Teilbereich des maschinellen Lernens, bei dem 
ein Agent durch Interaktion mit einer Umgebung eine optimale Strategie erlernt. 
Der Lernprozess basiert auf einem Belohnungssystem: Der Agent führt Aktionen aus, 
erhält daraufhin Belohnungen oder Bestrafungen und passt sein Verhalten 
entsprechend an. RL-Probleme werden typischerweise als Markov-Entscheidungsprozesse 
(MDP) modelliert, bestehend aus:
\begin{itemize}
    \item Zustand (State, $S$): Die aktuelle Situation der Umgebung.
    \item Aktion (Action, $A$): Eine Entscheidung, die der Agent treffen kann.
    \item Belohnung (Reward, $R$): Eine Rückmeldung, die die Qualitaten der gewählten Aktion bewertet.
    \item Übergangsmodell ($P(s' \vert s, a)$): Wahrscheinlichkeiten, dass der Zustand $s'$ nach einer Aktion $a$ im Zustand $s$ entsteht.
    \item Policy ($\pi(s)$): Die Strategie des Agenten zur Auswahl von Aktionen.
\end{itemize}
Das Ziel ist es, eine Optimale Policy $\pi^*$ zu lernen, die die kumulierte zukünftige Belohnung maximiert. 
Dafür gibt es verschiedene Methoden, darunter Q-Learning und Deep Q-Learning.

\subsection{Q-Learning}
Q-Learning ist ein wertbasierter RL-Algorithmus, der darauf abzielt, die Q-Werte 
für jede Zustands-Aktions-Kombination zu erlernen. Der Q-Wert $Q(s,a)$ repräsentiert 
die erwartete zukünftige Belohnung, wenn der Agent in Zustand $s$ Aktion $a$ wählt 
und danach der optimalen Strategie folgt.
Die Aktualisierung der Q-Werte erfolgt iterativ mit der Bellman-Gleichung:
\begin{equation}
    Q(s,a) \leftarrow Q(s,a) + \alpha (r + \gamma \max_{a'}Q(s',a') - Q(s,a))
\end{equation}
Hierbei sind:
\begin{itemize}
    \item $\alpha$ die Lernrate, die bestimmt, wie stark neue Informationen alte Werte überschreiben.
    \item $\gamma$ der Diskontfaktor, der die Gewichtung zukünftiger Belohnungen bestimmt.
    \item $r$ die unmittelbare Belohnung nach der Aktion $a$.
\end{itemize}
Da Q-Learning tabellarisch arbeitet, ist es nur für kleine Zustandsräume geeignet, 
da die Q-Tabelle bei vielen möglichen Zuständen und Aktionen schnell zu groß wird. 
In komplexeren Umgebungen ist daher eine neuronale Netzarchitektur notwendig - 
hier kommt Deep Q-Learning (DQN) ins Spiel.

\subsection{Deep Q-Learning}
Deep Q-Learning (DQN) erweitert Q-Learning durch den Einsatz eines neuronalen 
Netzwerks zur Approximation der Q-Werte, anstatt eine explizite Tabelle zu 
speichern. Dies erlaubt das Lernen in hochdimensionalen Zustandsräumen, die 
tabellarische Methoden überfordern würden.
Die Hauptbestandteile von DQN sind:
\begin{enumerate}
    \item Neuronales Netz als Q-Funktion
        \begin{itemize}
            \item Das Netz nimmt den Zustand $s$ als Eingabe und gibt geschätzte Q-Werte für alle möglichen Aktionen $a$ aus.
            \item Die Gewichte des Netzwerks werden durch Gradientenabstieg und einen Mean-Squared-Error (MSE)-Loss aktualisiert.
        \end{itemize}
    \item Erfahrungsspeicher (Experience Replay)
        \begin{itemize}
            \item Anstatt direkt mit den neuesten Erfahrungen zu trainieren, werden vergangene Erfahrungen $(s,a,r,s')$ in einem Speicher abgelegt.
        \end{itemize}
    \item Zielnetzwerk (Target Network)
        \begin{itemize}
            \item Zusätzlich zum Hauptnetzwerk existiert eine separate Kopie, die in regelmäßigen Abständen aktualisiert wird.
            \item Dies verhindert zu starke Schwankungen in den Q-Werten und stabilisiert das Training.
        \end{itemize}
\end{enumerate}
Die Aktualisierungsregel für DQN basiert auf dem Mean-Squared-Error zwischen dem 
vorhergesagten und dem Ziel-Q-Wert:
\begin{equation}
    L = \left(r + \gamma \max_{\alpha'}Q_{\text{target}}(s',a') - Q_{\text{current}}(s,a)\right)^2
\end{equation}
DQN ist besonders mächtig für komplexe Umgebungen, in denen eine tabellarische 
Q-Funktion nicht mehr praktikabel ist.

% \section{Verwandte Arbeiten}