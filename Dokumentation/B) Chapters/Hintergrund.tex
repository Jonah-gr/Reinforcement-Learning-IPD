\chapter{Hintergrund und theoretischer Rahmen}
\section{Das Iterierter Gefangenendilemma}
Das Gefangenendilemma ist eines der bekanntesten Probleme der Spieltheorie und 
beschreibt eine Situation, in der zwei Spieler unabhängig voneinander entscheiden 
müssen, ob sie kooperieren oder defektieren.\footcite{fudenberg1991gametheory} Die Auszahlung für jeden Spieler hängt 
sowohl von der eigenen Entscheidung als auch von der des Gegenübers ab. 
Die klassische Auszahlungsmatrix sieht dabei folgendermaßen aus:

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c}
            & Spieler B kooperiert & Spieler B defektiert\\
        \hline
        Spieler A kooperiert &  \makecell{(R, R) \\ Belohnung für Kooperation} & \makecell{(S, T) \\ A verliert, B gewinnt} \\
        \hline
        Spieler A defektiert &  \makecell{(T, S) \\ A gewinnt, B verliert} & \makecell{(P, P) \\ Bestrafung für gegenseitige Defektion} \\
    \end{tabular}
    \caption{Allgemeine Auszahlungsmatrix}
    \label{table:allgemeineauszahlungsmatrix}
\end{table}

Dabei gilt üblicherweise:
\begin{itemize}
    \item T (Temptation) > R (Reward) > P (Punishment) > S (Sucker's payoff)
    \item 2R > T + S, sodass sich gegenseitige Kooperation langfristig mehr lohnen würde als wechselseitige Ausnutzung.
\end{itemize}
Im einmaligen Gefangenendilemma ist die dominante Strategie, zu defektieren, da 
dies in jedem individuellen Fall die höhere Auszahlung sichert -- unabhängig von der 
Entscheidung des Gegenspielers. Dies führt jedoch zu einem sozial suboptimalen 
Ergebnis.

Im iterierten Gefangenendilemma wird das Spiel jedoch mehrfach hintereinander 
gespielt, sodass frühere Entscheidungen zukünftige Interaktionen beeinflussen können. 
Dadurch eröffnen sich neue Möglichkeiten für kooperative Strategien, bei denen 
Agenten versuchen, durch wechselseitige Zusammenarbeit langfristig höhere Erträge 
zu erzielen. Bekannte Strategien aus der Spieltheorie für das IPD sind beispielsweise:
\begin{itemize}
    \item ''Tit-for-Tat'' (Spiele das, was dein Gegner in der vorherigen Runde getan hat).
    \item ''Always Defect'' (Immer defektieren, um kurzfristig die höchste Auszahlung zu sichern).
\end{itemize}
% ''Tit-for-Tat'' (Spiele das, was dein Gegner in der vorherigen Runde getan hat).
% ''Always Defect'' (Immer defektieren, um kurzfristig die höchste Auszahlung zu sichern).
% ''Grim Trigger'' (Kooperiere, aber falls der Gegner einmal defektiert, defektiere für immer).

Die zentrale Forschungsfrage im IPD lautet daher: Ist es möglich, langfristige 
Kooperation zu etablieren, oder führt Eigennutz immer zu gegenseitiger Defektion? 
Diese Fragestellung ist besonders relevant für Reinforcement Learning-Agenten, da 
sie ihre Strategie durch wiederholte Interaktion und Belohnungsmechanismen erlernen.\footcite{axelrod1984cooperation}

\section{Reinforcement Learning}
Reinforcement Learning (RL) ist ein Teilbereich des maschinellen Lernens, bei dem 
ein Agent durch Interaktion mit einer Umgebung eine optimale Strategie erlernt. 
Der Lernprozess basiert auf einem Belohnungssystem: Der Agent führt Aktionen aus, 
erhält daraufhin Belohnungen oder Bestrafungen und passt sein Verhalten 
entsprechend an. RL-Probleme werden typischerweise als Markov-Entscheidungsprozesse 
modelliert, bestehend aus:
\begin{itemize}
    \item Zustand (State, $S$): Die aktuelle Situation der Umgebung.
    \item Aktion (Action, $A$): Eine Entscheidung, die der Agent treffen kann.
    \item Belohnung (Reward, $R$): Eine Rückmeldung, die die Qualitäten der gewählten Aktion bewertet.
    \item Übergangsmodell ($P(s' \vert s, a)$): Wahrscheinlichkeiten, dass der Zustand $s'$ nach einer Aktion $a$ im Zustand $s$ entsteht.
    \item Policy ($\pi(s)$): Die Strategie des Agenten zur Auswahl von Aktionen.
\end{itemize}
Das Ziel ist es, eine Optimale Policy $\pi^*$ zu lernen, die die kumulierte zukünftige Belohnung maximiert.\footcite{sutton2018reinforcement} 
Dafür gibt es verschiedene Methoden, darunter Q-Learning und Deep Q-Learning.

\subsection{Q-Learning}
Q-Learning ist ein wertbasierter RL-Algorithmus, der darauf abzielt, die Q-Werte 
für jede Zustands-Aktions-Kombination zu erlernen. Der Q-Wert $Q(s,a)$ repräsentiert 
die erwartete zukünftige Belohnung, wenn der Agent in Zustand $s$ Aktion $a$ wählt 
und danach der optimalen Strategie folgt.
Die Aktualisierung der Q-Werte erfolgt iterativ mit der Bellman-Gleichung\footcite{qlearning}:
\begin{equation}
    Q(s,a) \leftarrow Q(s,a) + \alpha (r + \gamma \max_{a'}Q(s',a') - Q(s,a))
\end{equation}
Hierbei sind:
\begin{itemize}
    \item $\alpha$ die Lernrate, die bestimmt, wie stark neue Informationen alte Werte überschreiben.
    \item $\gamma$ der Diskontfaktor, der die Gewichtung zukünftiger Belohnungen bestimmt.
    \item $r$ die unmittelbare Belohnung nach der Aktion $a$.
\end{itemize}
Da Q-Learning tabellarisch arbeitet, ist es nur für kleine Zustandsräume geeignet, 
da die Q-Tabelle bei vielen möglichen Zuständen und Aktionen schnell zu groß wird. 
In komplexeren Umgebungen ist daher eine neuronale Netzarchitektur notwendig -- 
hier kommt Deep Q-Learning ins Spiel.

\subsection{Deep Q-Learning}
Deep Q-Learning erweitert Q-Learning durch den Einsatz eines neuronalen 
Netzwerks zur Approximation der Q-Werte, anstatt eine explizite Tabelle zu 
speichern. Das Netz nimmt den Zustand $s$ als Eingabe und gibt geschätzte Q-Werte 
für alle möglichen Aktionen $a$ aus und die Gewichte des Netzwerks werden durch 
Gradientenabstieg aktualisiert. Anstatt aber das Netzwerk nach jedem Ereigniss zu 
trainieren, werden normalerweise vergangene Erfahrungen $(s,a,r,s')$ in einem 
Speicher abgelegt, um so die Q-Werte in einem großen Schritt zu verbessern.
Dazu wird zufällig aus diesem Speicher ein \textit{Batch} (ein Teil der Erfahrungen) 
ausgewählt und die Q-Werte des Netzwerks aktualisiert (\textit{experience replay}).
Zudem existiert ein Zielnetzwerk, das die Q-Werte der optimalen Strategie 
repräsentiert und eine Kopie des originalen Netzwerks ist, welches aber seltener
oder geringfügiger aktualisiert wird. Dies verhindert zu starke Schwankungen in den 
Q-Werten und stabilisiert das Training.\footcite{mnih2013playingatarideepreinforcement}

Deep Q-Learning ermöglicht das Lernen in hochdimensionalen Zustandsräumen, die 
tabellarische Methoden überfordern würden. \\ \\
Es gibt eine Vielzahl anderer Reinforcement-Learning-Algorithmen, darunter Policy-Gradient-Methoden 
(z. B. REINFORCE), Actor-Critic-Ansätze (z. B. A2C, PPO) und Monte-Carlo-Learning. 
Für meine Arbeit habe ich mich gezielt für 
Q-Learning und Deep Q-Learning entschieden, da sie einfach zu implementieren sind, nicht besonders 
rechenintensiv sind, aber trotzdem effektiv sind. Da das IPD selbst nicht sehr komplex ist, 
empfiehlt es sich, auch vergleichsweise simple bis moderate Reinforcement-Learning-Strategien zu verwenden.
% Zudem eignen sich solche wertebasierten Methoden besonders gut in diskreten Entscheidungsproblemen, wie auch 
% das Gefangenen-Dilemma eines ist.

%  da sie sich besonders gut für diskrete 
%  Entscheidungsprobleme wie das Gefangenen-Dilemma eignen. 
%  Das IPD ist nicht so komplex, dass 
%  eine kompliziertere und rechenintensivere Reinforcement-Learning-Strategie erforderlich wäre 
%  und dennoch können 